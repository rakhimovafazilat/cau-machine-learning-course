# -*- coding: utf-8 -*-
"""210013hw2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eNB0HbeITzoRob7ee8XxNxs0BH-oK9pd
"""

# Commented out IPython magic to ensure Python compatibility.
# Run some setup code for this notebook.
import random
import numpy as np
import matplotlib.pyplot as plt

# Some more magic so that the notebook will reload external python modules;
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2

w = np.random.rand(3, 2)
X = np.random.rand(5, 3)
y = np.random.rand(5, 2)

np.transpose(X).dot(X.dot(w) - y)

import numpy as np

data = np.array([[1.26515935, 1.40619743],
                 [2.07736013, 2.91620782],
                 [1.49368326, 1.85473737]])

# Now you can use the 'data' array

w = np.array([3, -2])

np.sign(w)

import numpy as np

my_array = np.array([1, -1])

w = np.array([1., 1.])

w = np.array([1., 1.])
w = np.vstack([w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]).T





# Gradient descent on the real data

def get_w_by_grad(X, Y, w_0, loss_mode='mse', reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05):
    if loss_mode == 'mse':
        loss_function = LossAndDerivatives.mse
        loss_derivative = LossAndDerivatives.mse_derivative
    elif loss_mode == 'mae':
        loss_function = LossAndDerivatives.mae
        loss_derivative = LossAndDerivatives.mae_derivative
    else:
        raise ValueError('Unknown loss function. Available loss functions: `mse`, `mae`')

    if reg_mode is None:
        reg_function = LossAndDerivatives.no_reg
        reg_derivative = LossAndDerivatives.no_reg_derivative # lambda w: np.zeros_like(w)
    elif reg_mode == 'l2':
        reg_function = LossAndDerivatives.l2_reg
        reg_derivative = LossAndDerivatives.l2_reg_derivative
    elif reg_mode == 'l1':
        reg_function = LossAndDerivatives.l1_reg
        reg_derivative = LossAndDerivatives.l1_reg_derivative
    else:
        raise ValueError('Unknown regularization mode. Available modes: `l1`, `l2`, None')


    w = w_0.copy()

    for i in range(n_steps):
        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)
        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)
        gradient_norm = np.linalg.norm(gradient)
        if gradient_norm > 5.:
            gradient = gradient / gradient_norm * 5.
        w -= lr * gradient

        if i % 25 == 0:
            print('Step={}, loss={},\ngradient values={}\n'.format(i, empirical_risk, gradient))
    return w

import numpy as np

w = np.ones((2, 1), dtype=float)

def get_w_by_grad(x_n, y_n, w, loss_mode='mse', reg_mode='l2', n_steps=250, learning_rate=0.01):
    for step in range(n_steps):

        if loss_mode == 'mse':
            loss_derivative = mse_derivative(x_n, y_n, w)
        else:
            raise ValueError(f"Unsupported loss_mode: {loss_mode}")
        if reg_mode == 'l2':
            reg_derivative = l2_reg_derivative(w)
            gradient = loss_derivative + reg_derivative
        else:
            raise ValueError(f"Unsupported reg_mode: {reg_mode}")


        w = w - learning_rate * gradient

    return w

# Comparing with sklearn

# Comparing with sklearn

from sklearn.linear_model import Ridge
import numpy as np

lr = Ridge(alpha=0.05)

import matplotlib.pyplot as plt
